import os
from dotenv import load_dotenv
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_openai import AzureOpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain.chat_models import AzureChatOpenAI
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain


# è¼‰å…¥ .env
load_dotenv()

def test_mongo_connection():
    MONGO_URI = os.getenv("MONGO_URI")

    if not MONGO_URI:
        print("âŒ æ‰¾ä¸åˆ° MONGODB_URI ç’°å¢ƒè®Šæ•¸")
        return
    try:
        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=3000)
        client.admin.command('ping')
        print("âœ… MongoDB é€£ç·šæˆåŠŸã€‚")
    except ConnectionFailure:
        print("âŒ MongoDB é€£ç·šå¤±æ•—")
    except Exception as e:
        print(f"âŒ MongoDB é€£ç·šéŒ¯èª¤: {e}")

# åˆå§‹åŒ– LLM
llm = AzureChatOpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
)

# åˆå§‹åŒ– Embedding æ¨¡å‹
embeddings = AzureOpenAIEmbeddings(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    azure_deployment=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME")
)

# è®€å– PDF æ–‡ä»¶
def load_pdf_documents():
    current_dir = os.path.dirname(__file__)
    parent_dir = os.path.dirname(current_dir)
    pdf_dir = os.path.join(parent_dir, "uploaded_files")
    all_docs = []
    if not os.path.exists(pdf_dir):
        print(f"âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™å¤¾ï¼š{pdf_dir}")
        return []
    for filename in os.listdir(pdf_dir):
        if filename.endswith(".pdf"):
            full_path = os.path.join(pdf_dir, filename)
            print(f"ğŸ“„ è¼‰å…¥ï¼š{filename}")
            loader = PyPDFLoader(full_path)
            docs = loader.load()
            all_docs.extend(docs)
    return all_docs

# åˆ†å‰²æ–‡ä»¶
def split_documents(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, add_start_index=True)
    return splitter.split_documents(documents)

# å»ºç«‹å‘é‡è³‡æ–™åº«
def create_vector_store(splits):
    return Chroma.from_documents(documents=splits, embedding=embeddings)

# å»ºç«‹é€²éš RAG éˆ
def create_advanced_rag_chain(retriever):
    system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you don't know. "
        "Use three sentences maximum and keep the answer concise.\n\n{context}"
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])
    qa_chain = create_stuff_documents_chain(llm, prompt)
    return create_retrieval_chain(retriever, qa_chain)

# å…ˆæ¸¬è©¦ MongoDB é€£ç·š
print("ğŸš€ æ¸¬è©¦ MongoDB é€£ç·š...")
test_mongo_connection()

# åˆå§‹åŒ– RAG ç³»çµ±
print("ğŸš€ åˆå§‹åŒ–é€²éš RAG ç³»çµ±...")
docs = load_pdf_documents()
splits = split_documents(docs)
vectorstore = create_vector_store(splits)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 2})
advanced_rag_chain = create_advanced_rag_chain(retriever)
print("âœ… é€²éš RAG ç³»çµ±åˆå§‹åŒ–å®Œæˆã€‚")

# æŸ¥è©¢å‡½å¼
def ask_question(question: str) -> str:
    response = advanced_rag_chain.invoke({"input": question})
    return response["answer"].strip()
